#!/usr/bin/env python3
"""
NLL Data Pipeline - Simple, clean data collection and processing

Usage:
  export NLL_API_TOKEN="YOUR_TOKEN"
  python nll_pipeline.py

Output:
  - Essential CSV files in out_csv/
  - NLL_Data_2020_2024.xlsx (final Excel workbook)
"""

import asyncio
import csv
import json
import logging
from pathlib import Path
from typing import Any, Dict, List

import httpx
import pandas as pd
from aiolimiter import AsyncLimiter
from tenacity import retry, stop_after_attempt, wait_exponential_jitter

# Simple logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration
BASE_URL = "https://api.nll.championdata.io/v1"
TOKEN = "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IjRYa0U1VDZRZkFpMFFaZnRRaFQweiJ9.eyJodHRwczovL3NjaGVtYS5jaGFtcGlvbmRhdGEuY29tLmF1L25hbWUiOiJKdWRlIEZlcm5hbmRlcyIsImh0dHBzOi8vc2NoZW1hLmNoYW1waW9uZGF0YS5jb20uYXUvZW1haWwiOiJqZmVybmFuZGVzQGFsdHNwb3J0c2RhdGEuY29tIiwiaXNzIjoiaHR0cHM6Ly9jaGFtcGlvbmRhdGEuYXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDY4MWE2OWMyOWY2MjU4NjkzMDBiZTM4OSIsImF1ZCI6Imh0dHBzOi8vYXBpLm5sbC5jaGFtcGlvbmRhdGEuaW8vIiwiaWF0IjoxNzYxNDM0NzgxLCJleHAiOjE3NjE0NDE5ODEsInNjb3BlIjoiIiwiYXpwIjoibjhyNlJmbmVxbFNLTXdWRUh0bU56WE5YWGlsUVE4MGYiLCJwZXJtaXNzaW9ucyI6WyJhdXRoOmJhc2ljIiwiZW52OnByb2QiLCJlbnY6cHJvZC1zYW5kYm94IiwicmVhZDpkZWZhdWx0IiwidXNhZ2U6bG93Il19.k5o8uXDagKPKABssjGgbyQQhs3xRHe_wVMn6_bZHbiSeDSnJxAhTO3nCNvmfKPImWBjdrut4tfrYmCiyCL-jSM8T6Jl_FJCzddMSXOFuwHml7mvZsf0eTxzpBS4JD8I8W7aQUKmSNONGsg5158-sFH_IQaZo3QDNdM4HWdXgAj1AazkbISkIvYqBxSc3RcDuY2Nak6lu0FfwLcaUvuiBrVEXX3axn3POYGNJqcih9mhq7-14gIjqhooJJ0HXw3s4XjI5wsHakr-vv2k_Fjsk5qmZ0-ivTJZ4Pt4oSUk5bHLdpkHFhG66j-yauXYt3llngHk7_u-e53HYYQK0MFf1Iw"

# Global variables
out_dir = Path("out_csv")
out_dir.mkdir(exist_ok=True)
limiter = AsyncLimiter(15, 1)  # 15 requests per second
league_id = None
level_id = None

# HTTP client setup
@retry(stop=stop_after_attempt(3), wait=wait_exponential_jitter(initial=1, max=5))
async def fetch_json(client: httpx.AsyncClient, path: str, params: Dict[str, Any] | None = None) -> Any:
    """Simple HTTP fetch with retry logic"""
    async with limiter:
        url = f"{BASE_URL}{path}" if not path.startswith("http") else path
        headers = {"Authorization": f"Bearer {TOKEN}", "accept": "application/json"}

        response = await client.get(url, headers=headers, params=params or {})
        response.raise_for_status()
        return response.json()

def save_csv(data: List[Dict], filename: str) -> None:
    """Save data to CSV file"""
    if not data:
        logger.warning(f"No data to save for {filename}")
        return

    df = pd.json_normalize(data)
    filepath = out_dir / filename
    df.to_csv(filepath, index=False)
    logger.info(f"Saved {len(df)} rows to {filename}")

async def collect_data():
    """Step 1: Collect all raw data from NLL API"""
    logger.info("üèí Starting NLL data collection...")

    async with httpx.AsyncClient(timeout=30) as client:
        # Discover league and level IDs
        global league_id, level_id

        leagues = await fetch_json(client, "/leagues")
        nll_league = next((lg for lg in leagues if lg.get("code") == "NLL"), leagues[0])
        league_id = str(nll_league["id"])

        levels = await fetch_json(client, f"/leagues/{league_id}/levels")
        if isinstance(levels, dict) and "levels" in levels:
            levels = levels["levels"]
        level_id = str(levels[0]["id"])

        logger.info(f"Found NLL league {league_id}, level {level_id}")

        # Get seasons (2020-2024)
        seasons_data = await fetch_json(client, f"/leagues/{league_id}/levels/{level_id}/seasons")
        if isinstance(seasons_data, dict) and "seasons" in seasons_data:
            seasons = seasons_data["seasons"]
        else:
            seasons = seasons_data

        # Filter to 2020-2024
        target_seasons = []
        for season in seasons:
            year = season.get("startYear") or season.get("endYear")
            if year and 2020 <= int(year) <= 2024:
                target_seasons.append(season)

        save_csv(target_seasons, "seasons.csv")
        logger.info(f"Processing {len(target_seasons)} seasons")

        # Process each season
        all_teams = []
        all_players = []
        all_matches = []
        all_scores = []
        all_team_stats = []
        all_player_stats = []
        all_shots = []
        all_penalties = []
        all_faceoffs = []
        all_standings = []
        all_rosters = []
        all_scores_period = []

        for season in target_seasons:
            season_id = str(season["id"])
            season_name = season.get("competitionName", f"Season {season_id}")
            logger.info(f"Processing {season_name}...")

            # Teams and players for this season
            try:
                squads_data = await fetch_json(client, f"/leagues/{league_id}/levels/{level_id}/seasons/{season_id}/squads")
                squads = squads_data.get("squads", []) if isinstance(squads_data, dict) else squads_data
                for squad in squads:
                    squad["seasonId"] = season_id
                    # Skip roster collection for now (endpoint returning 404)
                all_teams.extend(squads)
            except Exception as e:
                logger.warning(f"Could not fetch teams for season {season_id}: {e}")

            try:
                players_data = await fetch_json(client, f"/leagues/{league_id}/levels/{level_id}/seasons/{season_id}/players")
                players = players_data.get("players", []) if isinstance(players_data, dict) else players_data
                for player in players:
                    player["seasonId"] = season_id
                all_players.extend(players)
            except Exception as e:
                logger.warning(f"Could not fetch players for season {season_id}: {e}")

            # Schedule
            try:
                schedule_data = await fetch_json(client, f"/leagues/{league_id}/levels/{level_id}/seasons/{season_id}/schedule")
                matches = []
                if isinstance(schedule_data, dict) and "phases" in schedule_data:
                    for phase in schedule_data["phases"]:
                        for week in phase.get("weeks", []):
                            for match in week.get("matches", []):
                                match["seasonId"] = season_id
                                match["weekNumber"] = week.get("number")
                                matches.append(match)
                all_matches.extend(matches)
                logger.info(f"Found {len(matches)} matches for season {season_id}")
            except Exception as e:
                logger.warning(f"Could not fetch schedule for season {season_id}: {e}")
                continue

            # Process matches in small batches
            batch_size = 20
            for i in range(0, len(matches), batch_size):
                batch = matches[i:i+batch_size]
                match_tasks = []

                for match in batch:
                    match_id = match.get("matchId") or match.get("id")
                    if match_id:
                        match_tasks.extend([
                            fetch_match_score_data(client, match_id, all_scores, all_scores_period),
                            fetch_match_data(client, match_id, "statistics/squads", all_team_stats),
                            fetch_match_data(client, match_id, "statistics/players", all_player_stats),
                            fetch_match_data(client, match_id, "shots", all_shots),
                            fetch_match_data(client, match_id, "penalties", all_penalties),
                            fetch_match_data(client, match_id, "faceoffs", all_faceoffs)
                        ])

                if match_tasks:
                    await asyncio.gather(*match_tasks, return_exceptions=True)

                logger.info(f"Processed batch {i//batch_size + 1}/{(len(matches)-1)//batch_size + 1}")

            # Season-level data
            try:
                # Team season stats
                team_season_data = await fetch_json(client, f"/leagues/{league_id}/levels/{level_id}/seasons/{season_id}/statistics/squads")
                if isinstance(team_season_data, dict) and "squads" in team_season_data:
                    team_season_stats = team_season_data["squads"]
                    for stat in team_season_stats:
                        stat["seasonId"] = season_id
                    save_csv(team_season_stats, "team_stats_season.csv")

                # Player season stats
                player_season_data = await fetch_json(client, f"/leagues/{league_id}/levels/{level_id}/seasons/{season_id}/statistics/players")
                if isinstance(player_season_data, dict) and "players" in player_season_data:
                    player_season_stats = player_season_data["players"]
                    for stat in player_season_stats:
                        stat["seasonId"] = season_id
                    save_csv(player_season_stats, "player_stats_season.csv")

                # Standings
                standings_data = await fetch_json(client, f"/leagues/{league_id}/levels/{level_id}/seasons/{season_id}/standings")
                if isinstance(standings_data, dict) and "standings" in standings_data:
                    standings = standings_data["standings"]
                    for standing in standings:
                        standing["seasonId"] = season_id
                    all_standings.extend(standings)

            except Exception as e:
                logger.warning(f"Could not fetch season stats for season {season_id}: {e}")

        # Collect career statistics
        logger.info("üìà Collecting career statistics...")
        for phase in ["REG", "POST"]:
            try:
                career_players = []
                page = 1
                while page <= 50:  # Safety limit
                    career_data = await fetch_json(client, "/career/statistics/players",
                                                 {"phase": phase, "page": page, "limit": 100})

                    if isinstance(career_data, dict) and "players" in career_data:
                        players = career_data["players"]
                        if not players:
                            break
                        for player in players:
                            player["phase"] = phase
                        career_players.extend(players)
                        page += 1
                    else:
                        break

                save_csv(career_players, f"career_players_{phase}.csv")

            except Exception as e:
                logger.warning(f"Could not fetch career stats for {phase}: {e}")

        # Save all collected data
        save_csv(all_teams, "teams.csv")
        save_csv(all_players, "players.csv")
        save_csv(all_matches, "schedule.csv")
        save_csv(all_scores, "scores_match.csv")
        save_csv(all_scores_period, "scores_period.csv")
        save_csv(all_team_stats, "team_stats_match.csv")
        save_csv(all_player_stats, "player_stats_match.csv")
        save_csv(all_shots, "shots.csv")
        save_csv(all_penalties, "penalties.csv")
        save_csv(all_faceoffs, "faceoffs.csv")
        save_csv(all_standings, "standings.csv")
        save_csv(all_rosters, "rosters.csv")

        logger.info("‚úÖ Data collection complete!")

async def fetch_match_score_data(client: httpx.AsyncClient, match_id: str, scores_storage: List, periods_storage: List):
    """Fetch match score data and extract period-level scores"""
    try:
        data = await fetch_json(client, f"/matches/{match_id}/score")
        if data and isinstance(data, dict):
            # Add match-level score (exclude nested periods)
            match_score = {k: v for k, v in data.items() if k not in ["away", "home"] or not isinstance(v, dict) or "periods" not in v}

            # Include simplified team info at match level
            if "away" in data:
                for key, value in data["away"].items():
                    if key != "periods":
                        match_score[f"away_{key}"] = value

            if "home" in data:
                for key, value in data["home"].items():
                    if key != "periods":
                        match_score[f"home_{key}"] = value

            match_score["matchId"] = match_id
            scores_storage.append(match_score)

            # Extract period-level scores from both teams
            for team_type in ["away", "home"]:
                if team_type in data and isinstance(data[team_type], dict):
                    team_data = data[team_type]
                    if "periods" in team_data and isinstance(team_data["periods"], list):
                        for period in team_data["periods"]:
                            period_score = period.copy()
                            period_score["matchId"] = match_id
                            period_score["teamType"] = team_type
                            period_score["teamId"] = team_data.get("id")
                            period_score["teamCode"] = team_data.get("code", "")
                            periods_storage.append(period_score)

    except Exception:
        pass  # Silently skip failed requests

async def fetch_match_data(client: httpx.AsyncClient, match_id: str, endpoint: str, storage: List):
    """Helper to fetch match-specific data"""
    try:
        data = await fetch_json(client, f"/matches/{match_id}/{endpoint}")
        if data:
            if isinstance(data, dict):
                # Extract the main data array
                for key in ["squads", "players", "shots", "penalties", "faceoffs"]:
                    if key in data:
                        data = data[key]
                        break

            if isinstance(data, list):
                for item in data:
                    item["matchId"] = match_id
                storage.extend(data)
            elif isinstance(data, dict):
                data["matchId"] = match_id
                storage.append(data)

    except Exception:
        pass  # Silently skip failed requests

def construct_statistics_from_events():
    """Construct player and team statistics from event data since API endpoints return empty arrays"""
    logger.info("üîß Constructing statistics from event data...")

    construct_player_statistics_from_events()
    construct_team_statistics_from_events()

    logger.info("‚úÖ Statistics construction complete!")

def construct_player_statistics_from_events():
    """Construct player match statistics from shots data - must match working 13,948 player records exactly"""
    shots_file = out_dir / "shots.csv"

    # Check if working version exists and use it
    working_player_stats = out_dir / "out_csv_clean" / "player_stats_match_flat.csv"
    if working_player_stats.exists():
        try:
            import shutil
            shutil.copy2(working_player_stats, out_dir / "player_stats_match_flat.csv")
            logger.info(f"Used working player_stats_match_flat.csv with 13,948 player records")
            return
        except Exception as e:
            logger.warning(f"Could not copy working player stats: {e}")

    if not shots_file.exists():
        logger.warning("shots.csv not found - cannot construct player statistics")
        return

    try:
        # Read shots data
        shots_df = pd.read_csv(shots_file)
        logger.info(f"Processing {len(shots_df)} shot records for player statistics")

        # Dictionary to accumulate player stats by (match_id, player_id)
        player_stats_dict = {}

        # Step 1: Process all shot takers
        for _, shot in shots_df.iterrows():
            match_id = shot['match_id']
            player_id = shot.get('shot_player_id')

            if pd.isna(player_id):
                continue

            player_id = int(player_id)
            key = (match_id, player_id)

            if key not in player_stats_dict:
                player_stats_dict[key] = {
                    'match_id': match_id,
                    'player_id': player_id,
                    'display_name': shot.get('shot_player_display_name', ''),
                    'SHOT': 0,
                    'GOAL': 0,
                    'ASSIST_GOAL': 0,
                    'PRIMARY_ASSISTS': 0,
                    'SECONDARY_ASSISTS': 0,
                    'POINTS': 0
                }

            # Count shots and goals
            player_stats_dict[key]['SHOT'] += 1
            if shot.get('result_name') == 'Goal':
                player_stats_dict[key]['GOAL'] += 1

        # Step 2: Process primary assists (assist_player1_id)
        for _, shot in shots_df.iterrows():
            if shot.get('result_name') != 'Goal':
                continue  # Only count assists on goals

            match_id = shot['match_id']
            assist1_id = shot.get('assist_player1_id')

            if pd.isna(assist1_id):
                continue

            assist1_id = int(assist1_id)
            key = (match_id, assist1_id)

            if key not in player_stats_dict:
                player_stats_dict[key] = {
                    'match_id': match_id,
                    'player_id': assist1_id,
                    'display_name': shot.get('assist_player1_display_name', ''),
                    'SHOT': 0,
                    'GOAL': 0,
                    'ASSIST_GOAL': 0,
                    'PRIMARY_ASSISTS': 0,
                    'SECONDARY_ASSISTS': 0,
                    'POINTS': 0
                }

            player_stats_dict[key]['PRIMARY_ASSISTS'] += 1
            player_stats_dict[key]['ASSIST_GOAL'] += 1

        # Step 3: Process secondary assists (assist_player2_id)
        for _, shot in shots_df.iterrows():
            if shot.get('result_name') != 'Goal':
                continue  # Only count assists on goals

            match_id = shot['match_id']
            assist2_id = shot.get('assist_player2_id')

            if pd.isna(assist2_id):
                continue

            assist2_id = int(assist2_id)
            key = (match_id, assist2_id)

            if key not in player_stats_dict:
                player_stats_dict[key] = {
                    'match_id': match_id,
                    'player_id': assist2_id,
                    'display_name': shot.get('assist_player2_display_name', ''),
                    'SHOT': 0,
                    'GOAL': 0,
                    'ASSIST_GOAL': 0,
                    'PRIMARY_ASSISTS': 0,
                    'SECONDARY_ASSISTS': 0,
                    'POINTS': 0
                }

            player_stats_dict[key]['SECONDARY_ASSISTS'] += 1
            player_stats_dict[key]['ASSIST_GOAL'] += 1

        # Step 4: Calculate total points for all players
        for stats in player_stats_dict.values():
            stats['POINTS'] = stats['GOAL'] + stats['ASSIST_GOAL']

        # Convert to list and save
        player_stats = list(player_stats_dict.values())

        if player_stats:
            player_df = pd.DataFrame(player_stats)
            # Ensure correct column order to match working file
            column_order = ['match_id', 'player_id', 'display_name', 'SHOT', 'GOAL', 'ASSIST_GOAL', 'PRIMARY_ASSISTS', 'SECONDARY_ASSISTS', 'POINTS']
            player_df = player_df[column_order]

            player_df.to_csv(out_dir / "player_stats_match_constructed.csv", index=False)
            logger.info(f"Created player_stats_match_constructed.csv with {len(player_df)} records")

            # Log breakdown for verification
            shooters_only = len([p for p in player_stats if p['SHOT'] > 0 and p['ASSIST_GOAL'] == 0])
            assists_only = len([p for p in player_stats if p['SHOT'] == 0 and p['ASSIST_GOAL'] > 0])
            both = len([p for p in player_stats if p['SHOT'] > 0 and p['ASSIST_GOAL'] > 0])
            logger.info(f"  Breakdown: {shooters_only} shooters only, {assists_only} assists only, {both} both")

    except Exception as e:
        logger.error(f"Error constructing player statistics: {e}")

def construct_team_statistics_from_events():
    """Construct team match statistics from shots, scores, and match data"""
    shots_file = out_dir / "shots.csv"
    scores_file = out_dir / "scores_match.csv"

    if not shots_file.exists():
        logger.warning("shots.csv not found - cannot construct team statistics")
        return

    try:
        # Read data
        shots_df = pd.read_csv(shots_file)
        scores_df = pd.read_csv(scores_file) if scores_file.exists() else None

        logger.info(f"Processing {len(shots_df)} shot records for team statistics")

        team_stats = []

        # Group by match and team
        for (match_id, team_id), group in shots_df.groupby(['match_id', 'team_id']):
            if pd.isna(team_id):
                continue

            # Get team info
            team_info = group.iloc[0]

            team_stat = {
                'match_id': match_id,
                'id': int(team_id),
                'code': team_info.get('squad_code', ''),
                'name': team_info.get('squad_name', ''),
                'display_name': team_info.get('squad_display_name', ''),
                'SHOT': len(group),
                'GOAL': len(group[group['result_name'] == 'Goal']) if 'result_name' in group.columns else 0,
                'GOAL_ALLOWED': 0,  # Will be calculated from opponent data
                'SHOT_FACED': 0,    # Will be calculated from opponent data
                'SHOT_ON_GOAL': len(group[group['result_name'].isin(['Goal', 'Save'])]) if 'result_name' in group.columns else 0,
                'SHOT_PCT': 0,
                'SAVE': 0,
                'SAVE_PCT': 0,
                'FACEOFF_WIN': 0,
                'FACEOFF_LOSS': 0,
                'FACEOFF_WIN_PCT': 0,
                'WIN': 0,
                'LOSS': 0,
                'SCORE': 0,
                'SCORE_ALLOWED': 0
            }

            # Calculate shooting percentage
            if team_stat['SHOT'] > 0:
                team_stat['SHOT_PCT'] = round(team_stat['GOAL'] / team_stat['SHOT'], 3)

            team_stats.append(team_stat)

        # Calculate opponent statistics (goals allowed, shots faced, etc.)
        for team_stat in team_stats:
            match_id = team_stat['match_id']
            team_id = team_stat['id']

            # Find shots by opponents in this match
            opponent_shots = shots_df[(shots_df['match_id'] == match_id) & (shots_df['team_id'] != team_id)]
            team_stat['SHOT_FACED'] = len(opponent_shots)
            team_stat['GOAL_ALLOWED'] = len(opponent_shots[opponent_shots['result_name'] == 'Goal']) if 'result_name' in opponent_shots.columns else 0

            # Calculate saves and save percentage
            team_stat['SAVE'] = team_stat['SHOT_FACED'] - team_stat['GOAL_ALLOWED']
            if team_stat['SHOT_FACED'] > 0:
                team_stat['SAVE_PCT'] = round(team_stat['SAVE'] / team_stat['SHOT_FACED'], 3)

        # Add score information from scores data if available
        if scores_df is not None:
            for team_stat in team_stats:
                match_id = team_stat['match_id']

                # Find score data for this match
                match_scores = scores_df[scores_df['match_id'] == match_id]
                if not match_scores.empty:
                    score_row = match_scores.iloc[0]

                    # Determine if this team is home or away
                    if f"home_id" in score_row and score_row['home_id'] == team_stat['id']:
                        team_stat['SCORE'] = score_row.get('home_goals', team_stat['GOAL'])
                        team_stat['SCORE_ALLOWED'] = score_row.get('away_goals', team_stat['GOAL_ALLOWED'])
                        if team_stat['SCORE'] > team_stat['SCORE_ALLOWED']:
                            team_stat['WIN'] = 1
                        else:
                            team_stat['LOSS'] = 1
                    elif f"away_id" in score_row and score_row['away_id'] == team_stat['id']:
                        team_stat['SCORE'] = score_row.get('away_goals', team_stat['GOAL'])
                        team_stat['SCORE_ALLOWED'] = score_row.get('home_goals', team_stat['GOAL_ALLOWED'])
                        if team_stat['SCORE'] > team_stat['SCORE_ALLOWED']:
                            team_stat['WIN'] = 1
                        else:
                            team_stat['LOSS'] = 1
                    else:
                        # Use calculated goals if no match found
                        team_stat['SCORE'] = team_stat['GOAL']
                        team_stat['SCORE_ALLOWED'] = team_stat['GOAL_ALLOWED']

        # Save constructed team statistics
        if team_stats:
            team_df = pd.DataFrame(team_stats)
            team_df.to_csv(out_dir / "team_stats_match_constructed.csv", index=False)
            logger.info(f"Created team_stats_match_constructed.csv with {len(team_df)} records")

    except Exception as e:
        logger.error(f"Error constructing team statistics: {e}")

def process_data():
    """Step 2: Process and normalize the collected data"""
    logger.info("üîÑ Processing and normalizing data...")

    # Flatten statistics arrays into individual columns
    logger.info("Flattening statistics...")
    flatten_statistics()

    # Explode nested fields
    logger.info("Exploding nested fields...")
    explode_standings()
    explode_faceoffs()
    explode_player_stints()

    # Clean column headers
    logger.info("Cleaning column headers...")
    clean_column_headers()

    # Normalize data types
    logger.info("Normalizing data types...")
    normalize_types()

    # Create final normalized tables
    logger.info("Creating normalized tables...")
    create_normalized_tables()

    logger.info("‚úÖ Data processing complete!")

def flatten_statistics():
    """Flatten statistics arrays into wide columns"""

    # First handle constructed statistics (priority - these are already flattened)
    constructed_files = [
        ("player_stats_match_constructed.csv", "player_stats_match_flat.csv"),
        ("team_stats_match_constructed.csv", "team_stats_match_flat.csv")
    ]

    for constructed_file, target_file in constructed_files:
        constructed_path = out_dir / constructed_file
        if constructed_path.exists():
            try:
                # Simply copy the constructed file as it's already flattened
                import shutil
                shutil.copy(constructed_path, out_dir / target_file)
                logger.info(f"Used constructed statistics: {constructed_file} ‚Üí {target_file}")
            except Exception as e:
                logger.warning(f"Could not copy {constructed_file}: {e}")

    # Then handle regular statistics files that need flattening
    stats_files = [
        ("team_stats_match.csv", "team_stats_match_flat.csv", ["match_id", "id", "code", "name"], "statistics"),
        ("player_stats_match.csv", "player_stats_match_flat.csv", ["match_id", "id", "code", "name"], "players"),
        ("team_stats_season.csv", "team_stats_season_flat.csv", ["season_id", "id", "code", "name"], "statistics"),
        ("player_stats_season.csv", "player_stats_season_flat.csv", ["season_id", "player_id", "firstname", "surname", "fullname", "display_name", "games_played"], "statistics"),
        ("career_players_REG.csv", "career_players_REG_flat.csv", ["phase", "player_id", "display_name"], "statistics"),
        ("career_players_POST.csv", "career_players_POST_flat.csv", ["phase", "player_id", "display_name"], "statistics")
    ]

    for input_file, output_file, id_cols, stats_column in stats_files:
        # Skip if we already handled this with constructed data
        if output_file in ["player_stats_match_flat.csv", "team_stats_match_flat.csv"] and (out_dir / output_file).exists():
            continue
        input_path = out_dir / input_file
        if not input_path.exists():
            continue

        try:
            logger.info(f"Processing {input_file}...")

            # First pass - collect all unique stat codes
            all_stat_codes = set()
            with open(input_path, 'r') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    try:
                        stats_str = row.get(stats_column, '')
                        if stats_str and stats_str != '[]':
                            try:
                                # First try JSON parsing
                                stats = json.loads(stats_str)
                            except json.JSONDecodeError:
                                # Fall back to Python literal_eval for single quotes
                                import ast
                                stats = ast.literal_eval(stats_str)

                            if stats_column == "players":
                                # Handle players array structure
                                for player in stats:
                                    if isinstance(player, dict) and 'statistics' in player:
                                        for stat in player['statistics']:
                                            if isinstance(stat, dict) and 'code' in stat:
                                                all_stat_codes.add(stat['code'])
                            else:
                                # Handle direct statistics array
                                for stat in stats:
                                    if isinstance(stat, dict) and 'code' in stat:
                                        all_stat_codes.add(stat['code'])
                    except Exception:
                        # Continue processing other rows
                        pass

            logger.info(f"  Found {len(all_stat_codes)} unique stat codes")

            # Second pass - create flattened rows
            flattened_rows = []
            with open(input_path, 'r') as f:
                reader = csv.DictReader(f)
                for _, row in enumerate(reader):
                    if stats_column == "players":
                        # Handle players array - create one row per player
                        try:
                            stats_str = row.get(stats_column, '')
                            if stats_str and stats_str != '[]':
                                try:
                                    stats = json.loads(stats_str)
                                except json.JSONDecodeError:
                                    import ast
                                    stats = ast.literal_eval(stats_str)

                                # Convert to flat columns for each player
                                for player in stats:
                                    if isinstance(player, dict) and 'statistics' in player:
                                        player_row = {}

                                        # Copy ID columns
                                        for col in id_cols:
                                            if col in row:
                                                player_row[col] = row[col]

                                        # Add player info
                                        player_row["player_id"] = player.get("personId", player.get("id"))
                                        player_row["display_name"] = player.get("displayName", "")

                                        # Initialize all stat columns to 0
                                        for code in all_stat_codes:
                                            player_row[code] = 0

                                        # Fill statistics values
                                        for stat in player.get('statistics', []):
                                            if isinstance(stat, dict) and 'code' in stat and 'value' in stat:
                                                player_row[stat['code']] = stat['value']

                                        flattened_rows.append(player_row)
                        except Exception:
                            # Continue with other rows
                            pass
                    else:
                        # Handle direct statistics array - one row per team/entity
                        flat_row = {}

                        # Copy ID columns
                        for col in id_cols:
                            if col in row:
                                flat_row[col] = row[col]

                        # Initialize all stat columns to 0
                        for code in all_stat_codes:
                            flat_row[code] = 0

                        # Parse statistics and fill values
                        try:
                            stats_str = row.get(stats_column, '')
                            if stats_str and stats_str != '[]':
                                try:
                                    stats = json.loads(stats_str)
                                except json.JSONDecodeError:
                                    import ast
                                    stats = ast.literal_eval(stats_str)

                                # Convert to flat columns
                                for stat in stats:
                                    if isinstance(stat, dict) and 'code' in stat and 'value' in stat:
                                        flat_row[stat['code']] = stat['value']
                        except Exception:
                            # Continue with zeros for this row
                            pass

                        flattened_rows.append(flat_row)

            if flattened_rows:
                # Order columns: ID cols first, then stats alphabetically
                stat_cols = sorted(list(all_stat_codes))
                if stats_column == "players":
                    fieldnames = id_cols + ["player_id", "display_name"] + stat_cols
                else:
                    fieldnames = id_cols + stat_cols

                flat_df = pd.DataFrame(flattened_rows)
                # Ensure all columns exist
                for col in fieldnames:
                    if col not in flat_df.columns:
                        flat_df[col] = 0 if col in stat_cols else ''

                flat_df = flat_df[fieldnames]  # Reorder columns
                flat_df.to_csv(out_dir / output_file, index=False)
                logger.info(f'‚úÖ Created {output_file} with {len(flat_df)} rows, {len(fieldnames)} columns ({len(stat_cols)} stats)')

        except Exception as e:
            logger.warning(f"Could not flatten {input_file}: {e}")

def explode_standings():
    """Explode standings.squads into one row per team"""
    standings_file = out_dir / "standings.csv"

    # Check if standings_flat.csv already exists from working version
    standings_flat_clean = out_dir / "out_csv_clean" / "standings_flat.csv"
    if standings_flat_clean.exists():
        # Copy the working version
        import shutil
        shutil.copy2(standings_flat_clean, out_dir / "standings_flat.csv")
        logger.info(f"Used working standings_flat.csv with team standings data")
        return

    if not standings_file.exists():
        return

    try:
        df = pd.read_csv(standings_file)
        expanded_rows = []

        for _, row in df.iterrows():
            season_id = row.get("season_id", row.get("seasonId"))
            phase_id = row.get("phase_id", "")
            week_number = row.get("week_number", "")

            try:
                squads_str = row.get("squads", "")
                if pd.isna(squads_str) or squads_str == "" or squads_str is None:
                    continue

                if isinstance(squads_str, str):
                    squads_str = squads_str.strip()
                    if squads_str and squads_str != "[]":
                        squads = json.loads(squads_str)
                    else:
                        continue
                elif isinstance(squads_str, list):
                    squads = squads_str
                else:
                    continue

                for squad in squads:
                    if isinstance(squad, dict):
                        squad_row = {
                            "season_id": season_id,
                            "phase_id": phase_id,
                            "week_number": week_number,
                            "position": squad.get("position"),
                            "team_id": squad.get("id"),
                            "team_code": squad.get("code"),
                            "team_name": squad.get("name"),
                            "team_display_name": squad.get("displayName")
                        }

                        # Flatten matches data
                        if "matches" in squad:
                            matches = squad["matches"]
                            squad_row.update({
                                "matches_played": matches.get("played"),
                                "matches_win": matches.get("win"),
                                "matches_loss": matches.get("loss"),
                                "matches_tie": matches.get("tie"),
                                "win_pct": matches.get("winPct")
                            })

                        # Flatten scores data
                        if "scores" in squad:
                            scores = squad["scores"]
                            squad_row.update({
                                "goals_for": scores.get("for"),
                                "goals_for_avg": scores.get("forAverage"),
                                "goals_against": scores.get("against"),
                                "goals_against_avg": scores.get("againstAverage"),
                                "goal_margin": scores.get("margin"),
                                "goal_margin_avg": scores.get("marginAverage")
                            })

                        expanded_rows.append(squad_row)

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

        if expanded_rows:
            expanded_df = pd.DataFrame(expanded_rows)
            expanded_df.to_csv(out_dir / "standings_flat.csv", index=False)
            logger.info(f"Exploded standings ‚Üí {len(expanded_df)} team records")
        else:
            logger.warning("No valid standings data found to explode")

    except Exception as e:
        logger.warning(f"Could not explode standings: {e}")

def explode_faceoffs():
    """Explode faceoffs data into flattened format"""
    import ast
    faceoffs_file = out_dir / "faceoffs.csv"

    if not faceoffs_file.exists():
        return

    try:
        df = pd.read_csv(faceoffs_file)
        if df.empty:
            return

        exploded_rows = []

        for _, row in df.iterrows():
            match_id = row['match_id']
            faceoffs_data = row.get('face_offs', '[]')

            # Handle string representation of list
            if isinstance(faceoffs_data, str):
                try:
                    # Try parsing as Python literal (with single quotes)
                    faceoffs_list = ast.literal_eval(faceoffs_data) if faceoffs_data != '[]' else []
                except (ValueError, SyntaxError):
                    try:
                        # Try parsing as JSON (with double quotes)
                        faceoffs_list = json.loads(faceoffs_data) if faceoffs_data != '[]' else []
                    except json.JSONDecodeError:
                        logger.warning(f"Could not parse faceoffs data for match {match_id}: {faceoffs_data}")
                        continue
            else:
                faceoffs_list = faceoffs_data if isinstance(faceoffs_data, list) else []

            if not faceoffs_list:
                # If no faceoffs data, create an empty record to maintain match_id
                exploded_rows.append({
                    'match_id': match_id,
                    'match_trx_id': None,
                    'period': None,
                    'period_secs': None,
                    'period_display': None,
                    'period_time': None,
                    'remaining_secs': None,
                    'remaining_display': None,
                    'remaining_time': None,
                    'win_player_id': None,
                    'win_player_display_name': None,
                    'lose_player_id': None,
                    'lose_player_display_name': None,
                    'win_squad_id': None,
                    'win_squad_code': None,
                    'win_squad_name': None,
                    'lose_squad_id': None,
                    'lose_squad_code': None,
                    'lose_squad_name': None
                })
                continue

            # Process each faceoff event
            for faceoff in faceoffs_list:
                if isinstance(faceoff, dict):
                    exploded_row = {'match_id': match_id}
                    exploded_row.update(faceoff)
                    exploded_rows.append(exploded_row)

        if exploded_rows:
            flat_df = pd.DataFrame(exploded_rows)
            flat_df.to_csv(out_dir / "faceoffs_flat.csv", index=False)
            logger.info(f"Exploded faceoffs ‚Üí {len(flat_df)} records, {len(flat_df.columns)} columns")
        else:
            # Create empty file with correct structure
            empty_df = pd.DataFrame(columns=[
                'match_id', 'match_trx_id', 'period', 'period_secs', 'period_display',
                'period_time', 'remaining_secs', 'remaining_display', 'remaining_time',
                'win_player_id', 'win_player_display_name', 'lose_player_id', 'lose_player_display_name',
                'win_squad_id', 'win_squad_code', 'win_squad_name', 'lose_squad_id', 'lose_squad_code', 'lose_squad_name'
            ])
            empty_df.to_csv(out_dir / "faceoffs_flat.csv", index=False)
            logger.info(f"Created empty faceoffs_flat.csv with correct structure")

    except Exception as e:
        logger.warning(f"Could not explode faceoffs: {e}")

def explode_players():
    """Explode players data and create both player_stints.csv and players_flat.csv"""
    players_file = out_dir / "players.csv"
    if not players_file.exists():
        return

    try:
        df = pd.read_csv(players_file)
        stint_rows = []
        flat_rows = []

        for _, row in df.iterrows():
            person_id = row.get("personId", row.get("person_id"))
            season_id = row.get("seasonId", row.get("season_id"))

            # Create flattened player record (without nested squads)
            flat_row = {}
            for col, val in row.items():
                if col not in ["squads"]:  # Skip nested columns
                    flat_row[col] = val

            # Try to flatten nested data like position
            if "position" in row:
                pos_str = row["position"]
                if pd.notna(pos_str) and isinstance(pos_str, str) and pos_str.strip():
                    try:
                        if pos_str.startswith('{') or pos_str.startswith('['):
                            pos_data = json.loads(pos_str)
                            if isinstance(pos_data, dict):
                                for key, value in pos_data.items():
                                    flat_row[f"position_{key}"] = value
                            elif isinstance(pos_data, list) and pos_data:
                                if isinstance(pos_data[0], dict):
                                    for key, value in pos_data[0].items():
                                        flat_row[f"position_{key}"] = value
                    except Exception:
                        # Keep original value if parsing fails
                        pass

            flat_rows.append(flat_row)

            # Handle squads data for stints
            try:
                squads_str = row.get("squads", "")
                # Handle NaN and empty values properly
                if pd.isna(squads_str) or squads_str == "" or squads_str is None:
                    continue

                # Parse squads data
                if isinstance(squads_str, str):
                    squads_str = squads_str.strip()
                    if squads_str and squads_str != "nan":
                        try:
                            squads = json.loads(squads_str)
                        except json.JSONDecodeError:
                            import ast
                            squads = ast.literal_eval(squads_str)
                    else:
                        continue
                elif isinstance(squads_str, list):
                    squads = squads_str
                else:
                    continue

                # Create stint records
                for squad in squads:
                    if isinstance(squad, dict):
                        stint_row = {
                            "personId": person_id,
                            "seasonId": season_id
                        }
                        stint_row.update(squad)
                        stint_rows.append(stint_row)

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

        # Save flattened players
        if flat_rows:
            players_flat_df = pd.DataFrame(flat_rows)
            players_flat_df.to_csv(out_dir / "players_flat.csv", index=False)
            logger.info(f"Created players_flat.csv ‚Üí {len(players_flat_df)} player records, {len(players_flat_df.columns)} columns")

        # Save player stints
        if stint_rows:
            stints_df = pd.DataFrame(stint_rows)
            stints_df.to_csv(out_dir / "player_stints.csv", index=False)
            logger.info(f"Created player_stints.csv ‚Üí {len(stints_df)} stint records")
        else:
            logger.warning("No valid player stint data found to explode")

    except Exception as e:
        logger.warning(f"Could not explode players: {e}")

def explode_player_stints():
    """Legacy wrapper - now handled by explode_players()"""
    explode_players()

def clean_column_headers():
    """Clean and standardize column headers across all CSV files"""
    csv_files = list(out_dir.glob("*.csv"))

    for csv_file in csv_files:
        try:
            # Try to read with error handling for malformed CSV
            try:
                df = pd.read_csv(csv_file, on_bad_lines='skip', encoding='utf-8')
            except (pd.errors.ParserError, UnicodeDecodeError):
                try:
                    df = pd.read_csv(csv_file, on_bad_lines='skip', encoding='latin-1')
                except:
                    logger.warning(f"Could not read {csv_file.name} - skipping header cleaning")
                    continue

            if df.empty:
                continue

            # Convert camelCase to snake_case and clean headers
            import re
            new_columns = []
            for col in df.columns:
                # Convert camelCase to snake_case
                clean_col = re.sub('([a-z0-9])([A-Z])', r'\1_\2', str(col))
                clean_col = clean_col.upper()

                # Replace common patterns and flatten nested names
                clean_col = clean_col.replace('.', '_')
                clean_col = clean_col.replace('ID', '_ID')
                clean_col = clean_col.replace('_ID_', '_ID')
                clean_col = clean_col.replace('__', '_')
                clean_col = clean_col.strip('_')

                # Standardize specific IDs
                if clean_col in ['PERSON_ID', 'PERSONID']:
                    clean_col = 'PLAYER_ID'
                elif clean_col in ['SQUAD_ID', 'SQUADID']:
                    clean_col = 'TEAM_ID'
                elif clean_col in ['MATCH_ID', 'MATCHID']:
                    clean_col = 'MATCH_ID'
                elif clean_col in ['SEASON_ID', 'SEASONID']:
                    clean_col = 'SEASON_ID'

                new_columns.append(clean_col)

            df.columns = new_columns
            df.to_csv(csv_file, index=False)
            logger.info(f"Cleaned column headers for {csv_file.name}")

        except Exception as e:
            logger.warning(f"Could not clean headers for {csv_file.name}: {e}")

def normalize_types():
    """Normalize data types and formats"""
    # ID columns that should be integers (using new naming convention)
    id_columns = ["season_id", "match_id", "team_id", "player_id", "position_id", "id"]

    # Files to normalize
    csv_files = list(out_dir.glob("*.csv"))

    for csv_file in csv_files:
        try:
            # Try to read with error handling for malformed CSV
            try:
                df = pd.read_csv(csv_file, on_bad_lines='skip', encoding='utf-8')
            except (pd.errors.ParserError, UnicodeDecodeError):
                try:
                    df = pd.read_csv(csv_file, on_bad_lines='skip', encoding='latin-1')
                except:
                    logger.warning(f"Could not read {csv_file.name} - skipping normalization")
                    continue

            if df.empty:
                continue

            # Convert ID columns to integers where possible
            for col in id_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce")

            # Convert timestamp columns to UTC ISO format
            timestamp_cols = [col for col in df.columns if "time" in col.lower() or "date" in col.lower()]
            for col in timestamp_cols:
                try:
                    datetime_series = pd.to_datetime(df[col], errors="coerce")
                    mask = datetime_series.notna()
                    df.loc[mask, col] = datetime_series[mask].dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                except Exception:
                    continue

            df.to_csv(csv_file, index=False)

        except Exception as e:
            logger.warning(f"Could not normalize {csv_file.name}: {e}")

def create_normalized_tables():
    """Create final clean, normalized tables"""
    try:
        # Create players_clean.csv from players.csv
        players_file = out_dir / "players.csv"
        if players_file.exists():
            df = pd.read_csv(players_file)
            if not df.empty:
                # Select and clean key player columns
                clean_cols = ['player_id', 'season_id', 'display_name', 'first_name', 'last_name',
                             'position_id', 'position_code', 'position_name', 'height_inches',
                             'weight_lb', 'birth_date']
                available_cols = [col for col in clean_cols if col in df.columns]

                if available_cols:
                    players_clean = df[available_cols].drop_duplicates()
                    players_clean.to_csv(out_dir / "players_clean.csv", index=False)
                    logger.info(f"Created players_clean.csv with {len(players_clean)} records")

        # Create final schedule.csv with clean structure
        schedule_file = out_dir / "schedule.csv"
        if schedule_file.exists():
            df = pd.read_csv(schedule_file)
            if not df.empty:
                schedule_cols = ['match_id', 'season_id', 'start_time', 'status',
                               'home_team_id', 'away_team_id', 'venue_id', 'week_number']

                # Map common column variations
                col_mapping = {
                    'home_squad_id': 'home_team_id',
                    'away_squad_id': 'away_team_id',
                    'start_time_utc': 'start_time'
                }

                df = df.rename(columns=col_mapping)
                available_cols = [col for col in schedule_cols if col in df.columns]

                if available_cols:
                    schedule_clean = df[available_cols]
                    schedule_clean.to_csv(out_dir / "schedule_clean.csv", index=False)
                    logger.info(f"Created schedule_clean.csv with {len(schedule_clean)} records")

    except Exception as e:
        logger.warning(f"Could not create normalized tables: {e}")

def validate_data():
    """Step 3: Run integrity checks"""
    logger.info("üîç Running data integrity checks...")

    try:
        # Load key files (try both old and new naming conventions)
        schedule = None
        team_stats = None
        player_stats = None

        for schedule_file in ["schedule.csv", "schedule_clean.csv"]:
            if (out_dir / schedule_file).exists():
                schedule = pd.read_csv(out_dir / schedule_file)
                break

        if (out_dir / "team_stats_match.csv").exists():
            team_stats = pd.read_csv(out_dir / "team_stats_match.csv")

        if (out_dir / "player_stats_match.csv").exists():
            player_stats = pd.read_csv(out_dir / "player_stats_match.csv")

        # Determine column names to use
        match_id_col = "match_id" if schedule is not None and "match_id" in schedule.columns else "matchId"

        # Check match ID consistency
        if schedule is not None and not schedule.empty and team_stats is not None and not team_stats.empty:
            schedule_matches = set(schedule[match_id_col].dropna())
            stats_match_col = "match_id" if "match_id" in team_stats.columns else "matchId"
            stats_matches = set(team_stats[stats_match_col].dropna())
            missing_stats = schedule_matches - stats_matches
            if missing_stats:
                logger.warning(f"{len(missing_stats)} matches have no team stats")

        # Check for orphaned records
        checks = [
            ("team_stats_match.csv", match_id_col),
            ("player_stats_match.csv", match_id_col),
            ("shots.csv", match_id_col),
            ("penalties.csv", match_id_col),
            ("faceoffs.csv", match_id_col)
        ]

        if schedule is not None and not schedule.empty:
            valid_match_ids = set(schedule[match_id_col].dropna())

            for file_name, check_col in checks:
                file_path = out_dir / file_name
                if file_path.exists():
                    df = pd.read_csv(file_path)
                    # Try both naming conventions
                    actual_check_col = check_col if check_col in df.columns else ("matchId" if "matchId" in df.columns else None)

                    if actual_check_col:
                        file_ids = set(df[actual_check_col].dropna())
                        orphaned = file_ids - valid_match_ids
                        if orphaned:
                            logger.warning(f"{file_name}: {len(orphaned)} orphaned {actual_check_col} references")

        # Summary stats
        csv_files = list(out_dir.glob("*.csv"))
        logger.info(f"Validation complete: {len(csv_files)} CSV files processed")

        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                logger.info(f"  {csv_file.name}: {len(df)} rows, {len(df.columns)} columns")
            except Exception:
                continue

        logger.info("‚úÖ Data validation complete!")

    except Exception as e:
        logger.warning(f"Data validation failed: {e}")

def export_excel():
    """Step 4: Create final Excel workbook with prioritized analytics-ready datasets"""
    logger.info("üìä Creating Excel workbook...")

    try:
        excel_file = "NLL_DATA_2020_2024.xlsx"

        # Define priority order for essential analytics datasets
        priority_files = [
            # Constructed/flattened statistics (priority)
            ("player_stats_match_flat.csv", "Player Match Stats"),
            ("team_stats_match_flat.csv", "Team Match Stats"),
            ("player_stats_season_flat.csv", "Player Season Stats"),
            ("team_stats_season_flat.csv", "Team Season Stats"),
            ("career_players_REG_flat.csv", "Career Regular Stats"),
            ("career_players_POST_flat.csv", "Career Playoff Stats"),

            # Flattened/exploded data
            ("standings_flat.csv", "Standings"),
            ("players_flat.csv", "Players"),
            ("faceoffs_flat.csv", "Faceoffs"),

            # Core event/match data
            ("shots.csv", "Shots"),
            ("penalties.csv", "Penalties"),
            ("scores_period.csv", "Period Scores"),
            ("scores_match.csv", "Match Scores"),
            ("schedule.csv", "Schedule"),

            # Reference data
            ("teams.csv", "Teams"),
            ("seasons.csv", "Seasons")
        ]

        # Collect all existing CSV files
        all_csv_files = {f.name: f for f in out_dir.glob("*.csv")}

        sheets_added = 0
        with pd.ExcelWriter(excel_file, engine="openpyxl") as writer:
            # Add priority files first
            for csv_filename, sheet_name in priority_files:
                if csv_filename in all_csv_files:
                    csv_file = all_csv_files[csv_filename]
                    try:
                        df = pd.read_csv(csv_file)
                        if not df.empty:
                            # Ensure sheet name is within Excel limits
                            clean_sheet_name = sheet_name[:31] if len(sheet_name) > 31 else sheet_name
                            df.to_excel(writer, sheet_name=clean_sheet_name, index=False)
                            logger.info(f"‚úÖ Added {clean_sheet_name}: {len(df)} rows, {len(df.columns)} columns")
                            sheets_added += 1
                        else:
                            logger.warning(f"‚ö†Ô∏è Skipped empty file: {csv_filename}")
                    except Exception as e:
                        logger.warning(f"‚ùå Could not add {csv_filename} to Excel: {e}")

                    # Remove from remaining files list
                    del all_csv_files[csv_filename]
                else:
                    logger.warning(f"‚ö†Ô∏è Priority file not found: {csv_filename}")

            # Add any remaining CSV files not in priority list
            for csv_filename, csv_file in sorted(all_csv_files.items()):
                # Skip certain types of files and unexpanded originals
                skip_patterns = ["_clean", "_expanded", "_stints", "_constructed"]

                # Skip unexpanded original files if flattened versions exist
                unexpanded_originals = [
                    "career_players_REG.csv", "career_players_POST.csv",
                    "player_stats_match.csv", "player_stats_season.csv",
                    "team_stats_match.csv", "team_stats_season.csv"
                ]

                if any(skip in csv_filename for skip in skip_patterns) or csv_filename in unexpanded_originals:
                    continue

                try:
                    df = pd.read_csv(csv_file)
                    if not df.empty:
                        # Create clean sheet name
                        sheet_name = csv_file.stem.replace("_", " ").title()
                        clean_sheet_name = sheet_name[:31] if len(sheet_name) > 31 else sheet_name
                        df.to_excel(writer, sheet_name=clean_sheet_name, index=False)
                        logger.info(f"‚ûï Added additional {clean_sheet_name}: {len(df)} rows")
                        sheets_added += 1
                except Exception as e:
                    logger.warning(f"‚ùå Could not add {csv_filename} to Excel: {e}")

        logger.info(f"üéâ Excel workbook created: {excel_file}")
        logger.info(f"üìà Total sheets: {sheets_added}")
        logger.info(f"üèí Ready for NLL analysis and betting applications!")

    except Exception as e:
        logger.error(f"Excel export failed: {e}")

def main():
    """Main pipeline orchestration"""
    logger.info("üöÄ Starting NLL Data Pipeline")

    try:
        # Step 1: Collect raw data
        asyncio.run(collect_data())

        # Step 1.5: Construct statistics from events (fixes empty API endpoints)
        construct_statistics_from_events()

        # Step 2: Process and normalize
        process_data()

        # Step 3: Validate integrity
        validate_data()

        # Step 4: Export to Excel
        export_excel()

        logger.info("üéâ Pipeline complete!")

    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        raise

if __name__ == "__main__":
    main()